/*
 * Copyright (C) 2010 Minwoo Jeong (minwoo.j@gmail.com).
 * This file is part of the "TriCRF" distribution.
 * http://github.com/minwoo/TriCRF/
 * This software is provided under the terms of LGPL.
 */

/// max headers
#include "TriCRF3.h"
#include "Evaluator.h"
#include "Utility.h"
#include "LBFGS.h"
/// standard headers
#include <cassert>
#include <cfloat>
#include <cmath>
#include <limits>
#include <algorithm>
#include <stdexcept>
#include <iostream>
#include <fstream>

/// for fast accessing the element of matrixes
#define MAT3(I, X, Y)			((m_state_size * m_state_size * (I)) + (m_state_size * (X)) + Y)
#define MAT2(I, X)				((m_state_size * (I)) + X)
#define MATZ2(Z, I, X)		((m_seq_size * m_state_size * Z) + (m_state_size * (I)) + X)
/// for fast accessing the element of matrixes (in TriCRF2)
#define TCRF2_MAT3(SIZE, I, X, Y)			((SIZE * SIZE * (I)) + (SIZE * (X)) + Y)
#define TCRF2_MAT2(SIZE, I, X)				((SIZE * (I)) + X)
/// for fast accessing the element of matrixes (in TriCRF3)
#define ZMAT3(Z, I, X, Y)	((m_state_size[Z] * m_state_size[Z] * (I)) + (m_state_size[Z] * (X)) + Y)
#define ZMAT2(Z, I, X)		((m_state_size[Z] * (I)) + X)
#define MAT(I, X)				((m_state_size2 * (I)) + X)

using namespace std;

namespace tricrf {

///////////////////////////////////////////////////////////////////////////////////////////////////////////////
//
// MODEL-3: Model1 + shared common feature (~ Model1 + Model2)
//
///////////////////////////////////////////////////////////////////////////////////////////////////////////////

/** Constructor.
*/
TriCRF3::TriCRF3() {
	m_default_oid = 0;
	m_topic_size = 0;
}

/** Constructor with logger.
*/
TriCRF3::TriCRF3(Logger *logger) {
	setLogger(logger);
	logger->report(2, MAX_HEADER);
	logger->report(2, ">> Triangular-chain Conditional Random Fields (Model1) << \n\n");
	m_default_oid = 0;
	m_topic_size = 0;
}

void TriCRF3::clear() {
	for (size_t i = 0; i < m_topic_size; i++)
		m_ParamSeq[i].clear();
	m_ParamSeq.clear();
	m_ParamTopic.clear();
	m_Param.clear();
	m_state_size.clear();
}

void TriCRF3::initializeModel() {
	for (size_t i = 0; i < m_topic_size; i++)
		m_ParamSeq[i].initialize();
	m_ParamTopic.initialize();
	m_Param.initialize();
}

/** Save the model.
	@param filename file to be saved 
	@return success or fail
*/
bool	TriCRF3::saveModel(const std::string& filename) {
	/// Checking the error
	if (filename == "")
		return false;

	timer stop_watch;
	logger->report("[Model saving]\n");

	/// file stream
    ofstream f(filename.c_str());
    f.precision(20);
    if (!f)
        throw runtime_error("unable to open file to write");

    /// header
    f << "# MAX: A C++ Library for Structured Prediction" << endl;
	f << "# TriCRF3 Model file (text format)" << endl;
	f << "# Do not edit this file" << endl;
	f << "# " << endl << ":" << endl;
	
	if (!m_ParamTopic.save(f))
		return false;
	for (size_t i = 0; i < m_topic_size; i++) {
		if (!m_ParamSeq[i].save(f))
			return false;
	}
	if (!m_Param.save(f)) 
		return false;
		
	map<pair<size_t, size_t>, size_t>::iterator it = m_Mapping.begin();
	for ( ; it != m_Mapping.end(); it++) {
		f << it->first.first << " " << it->first.second << " " << it->second << endl;
	}
	
	f.close();

	logger->report("  saving time = \t%.3f\n\n", stop_watch.elapsed());

	return true;
}

/** Load the model.
	@param filename file to be loaded
	@return success or fail
*/
bool TriCRF3::loadModel(const std::string& filename) {
	/// Checking the error
	if (filename == "")
		return false;

	timer stop_watch;
	logger->report("[Model loading]\n");

	/// file stream
    ifstream f(filename.c_str());
    f.precision(20);
    if (!f)
        throw runtime_error("fail to open model file");

    /// header
	size_t count = 0;
    string line;
    getline(f, line);
    while (line.empty() || line[0] == '#') {
		if (count == 1) {
			vector<string> tok = tokenize(line);
			if (tok.size() < 2 || tok[1] != "TriCRF3") {
				logger->report("|Error| Invalid model files ... \n");
				return false;
			}
		}
        getline(f, line);
		count++;
	}

	if (!m_ParamTopic.load(f))
		return false;
	logger->report("  >>Parameters for topic features\n");
	m_ParamTopic.print(logger);

	m_topic_size = m_ParamTopic.sizeStateVec();
	m_ParamSeq.resize(m_topic_size);
	for (size_t i = 0; i < m_topic_size; i++) {
		if (!m_ParamSeq[i].load(f))
			return false;
		logger->report("  >>Parameters for %d plane\n", i);
		m_ParamSeq[i].print(logger);
	}
	if (!m_Param.load(f))
		return false;
	logger->report("  >>Parameters for common features\n");
	m_Param.print(logger);	
		
	m_Mapping.clear();
	m_RMapping.clear();
	while (getline(f, line)) {	
		
		vector<string> tok = tokenize(line);
		assert (tok.size() == 3);
		m_Mapping[make_pair(atoi(tok[0].c_str()), atoi(tok[1].c_str()))] = atoi(tok[2].c_str());
	}
	
	f.close();
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());

	m_topic_size = m_ParamTopic.sizeStateVec();
	//m_Param.clear(true);

	for (size_t i = 0; i < m_ParamTopic.sizeStateVec(); i++) {
		m_ParamSeq[i].makeStateIndex();
		m_state_size.push_back(m_ParamSeq[i].sizeStateVec());
	}
	//m_ParamTopic.makeStateIndex(false);
	m_Param.makeStateIndex();
	
	return true;
}

/**	Read the data from file
*/
void TriCRF3::readTrainData(const string& filename) {

	m_Mapping.clear();
	m_RMapping.clear();
	
	/// File stream
	string line;
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");

	size_t seq_count = 0;		
	size_t topic_id = 0;
	// Make a state space Y
	while ( getline(f, line) ) {
		if ( !line.empty() ) {
			vector<string> tokens = tokenize(line);
			if (tokens.size() > 0) {
				size_t index = 0;
				string fstr(tokens[index]);
				vector<string> tok = tokenize(fstr, ":");
				float fval = 1.0;
				if (tok.size() > 1) {
					fval = atof(tok[1].c_str());	///< feature value
					fstr = tok[0];
				}
				
				++seq_count;
				if (seq_count == 1) { ///< this is a topic 
					size_t n_topic = m_ParamTopic.sizeStateVec();			
					topic_id = m_ParamTopic.addNewState(fstr);	// outcome id
					if (topic_id >= n_topic) { ///< new topic label
						Parameter param;
						m_ParamSeq.push_back(param);
					}				
				} else {
					size_t yz = m_ParamSeq[topic_id].addNewState(fstr);	// outcome id
					size_t y = m_Param.addNewState(fstr); // shared common feature -- for domain adaptation
					pair<size_t, size_t> key = make_pair(topic_id, y);
					if (m_Mapping.find(key) == m_Mapping.end())
						m_Mapping[key] = yz;
				}
								
			}
		
		}
		else
			seq_count = 0;
	}
	
	f.clear();
	f.seekg(0, ios::beg);
	
	
	/// initializing
	TriStringSequence triseq;
	size_t count = 0;
	string prev_label = "";
	//string topic;
	timer stop_watch;
	logger->report("[Training data file loading]\n");
	m_TrainSet.clear();
	m_TrainSetCount.clear();

	/// To reduce the storage and computation
	map<vector<vector<string> >, size_t> train_data_map;
	vector<vector<string> > token_list;

	seq_count = 0;
	while (getline(f,line)) {
		vector<string> tokens = tokenize(line, " \t");
		if (line.empty() || tokens.size() <= 0) {	 ///< sequence break
			/*
			TriSequence tt;
			tt.topic.label = triseq.topic.label;
			tt.topic.fval = triseq.topic.fval;
			for (size_t i = 0; i < triseq.seq.size(); i++) {
				Event e;
				e.label = triseq.seq[i].label;
				e.fval = triseq.seq[i].fval;
				tt.seq.push_back(e);
			}
			*/
			if (train_data_map.find(token_list) == train_data_map.end()) {
				m_TrainSet.append(triseq);
				train_data_map.insert(make_pair(token_list, m_TrainSetCount.size()));
				m_TrainSetCount.push_back(1.0);
				
				//vector<TriSequence> temp;
				//temp.push_back(tt);
				//m_TrainLabelSet.push_back(temp);
			} else {
				m_TrainSetCount[train_data_map[token_list]] += 1.0;

				//m_TrainLabelSet[train_data_map[token_list]].push_back(tt);
			}
			triseq.seq.clear();
			token_list.clear();
			prev_label = "";
			seq_count = 0;
			++count;
		} else {
			++seq_count;
			token_list.push_back(tokens);
						
			if (seq_count == 1) { ///< this is a topic 
				size_t n_topic = m_ParamTopic.sizeStateVec();
				triseq.topic = packEvent(tokens, &m_ParamTopic);	///< wanrning: There are no common element in topic classes and sequence classes.
				//topic = tokens[0];
				/// testing for a new topic label
				if (triseq.topic.label >= n_topic) { ///< new topic label
					Parameter param;
					m_ParamSeq.push_back(param);
				}
			} else {
				StringEvent ev = packStringEvent(tokens,  &m_ParamSeq[triseq.topic.label]);	///< observation features
				triseq.seq.push_back(ev);	///< append
				Event ev2 = packEvent2(tokens);
				
				/// State transition features
				/// This can be extended to state-dependent observation features. (See Sutton and McCallum, 2006)
				if (prev_label != "") {
					size_t pid = m_ParamSeq[triseq.topic.label].addNewObs("@" + prev_label);
					
					/*
					for (size_t i = 0; i < m_ParamSeq[triseq.topic.label].sizeStateVec(); i++) {
						if (i == ev.label) 
							m_ParamSeq[triseq.topic.label].updateParam(ev.label, pid, ev.fval);							
						else
							m_ParamSeq[triseq.topic.label].updateParam(i, pid, 0.0);
					}*/
					
					m_ParamSeq[triseq.topic.label].updateParam(ev.label, pid, ev.fval);
					
					pid = m_Param.addNewObs("@" + prev_label);
					/*for (size_t i = 0; i < m_Param.sizeStateVec(); i++) {
						if (i == ev2.label)
							m_Param.updateParam(ev2.label, pid, ev2.fval);
						else
							m_Param.updateParam(i, pid, 0.0);
					}*/		
					m_Param.updateParam(ev2.label, pid, ev2.fval);
								
				}

				//prev_label = tokens[0];
				string fstr(tokens[0]);
				vector<string> tok = tokenize(fstr, ":");
				double fval = 1.0;
				if (tok.size() > 1) {
					fval = atof(tok[1].c_str());	///< feature value
					fstr = tok[0];
				}				
				prev_label = fstr;
			}
		}	// else

	}	// while
	m_topic_size = m_ParamTopic.sizeStateVec();
	
	for (size_t i = 0; i < m_topic_size; i++) {
		m_ParamSeq[i].endUpdate();
	}
	m_ParamTopic.endUpdate();
	//m_Param.clear(true);
	m_Param.endUpdate();

	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());
	
	for (size_t i = 0; i < m_ParamTopic.sizeStateVec(); i++) {
		m_ParamSeq[i].makeStateIndex();
		m_state_size.push_back(m_ParamSeq[i].sizeStateVec());
	}

	//m_ParamTopic.makeStateIndex(false);
	m_Param.makeStateIndex();

}

/**	Read the data from file
*/
void TriCRF3::readDevData(const string& filename) {

	/// File stream
	string line;
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");
	
	/// initializing
	TriStringSequence triseq;
	size_t count = 0;
	string prev_label = "";
	string topic;
	timer stop_watch;
	logger->report("[Dev data file loading]\n");
	m_DevSet.clear();
	m_DevSetCount.clear();

	/// To reduce the storage and computation
	map<vector<vector<string> >, size_t> dev_data_map;
	vector<vector<string> > token_list;

	size_t seq_count = 0;
	while (getline(f,line)) {
		vector<string> tokens = tokenize(line, " \t");
		if (line.empty() || tokens.size() <= 0) {	 ///< sequence break
			if (dev_data_map.find(token_list) == dev_data_map.end()) {
				m_DevSet.append(triseq);
				dev_data_map.insert(make_pair(token_list, m_DevSetCount.size()));
				m_DevSetCount.push_back(1.0);
			} else {
				m_DevSetCount[dev_data_map[token_list]] += 1.0;
			}
			triseq.seq.clear();
			token_list.clear();
			prev_label = "";
			seq_count = 0;
			++count;
		} else {
			++seq_count;
			token_list.push_back(tokens);
			if (seq_count == 1) { ///< this is a topic 
				triseq.topic = packEvent(tokens, &m_ParamTopic, true);	///< wanrning: There are no common element in topic classes and sequence classes.
				topic = tokens[0];
			} else {
				size_t z = (triseq.topic.label < m_ParamTopic.sizeStateVec() ? triseq.topic.label : m_default_oid);
				StringEvent ev = packStringEvent(tokens,  &m_ParamSeq[z], true);	///< observation features
				triseq.seq.push_back(ev);	///< append

				prev_label = tokens[0];
			}
		}	// else

	}	// while

	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());

}


void TriCRF3::calculateEdge() {
	vector<double*> theta_seq;
	for (size_t z = 0; z < m_topic_size; z++) 
		theta_seq.push_back(m_ParamSeq[z].getWeight());
	double* theta_topic = m_ParamTopic.getWeight();		
	
	/// Factor matrix initialization
	m_M.resize(m_topic_size);
	for (size_t z = 0; z < m_topic_size; z++) {
		m_M[z].resize(m_state_size[z] * m_state_size[z]);
		fill(m_M[z].begin(), m_M[z].end(), 1.0);
	}

	/// Calculation
	for (size_t z = 0; z < m_topic_size; z++) {
		vector<StateParam>::iterator iter = m_ParamSeq[z].m_StateIndex.begin();
		for (; iter != m_ParamSeq[z].m_StateIndex.end(); ++iter) {
			m_M[z][ZMAT2(z, iter->y1,iter->y2)] *= exp(theta_seq[z][iter->fid] * iter->fval);	 
		}

	} ///< for each z

	double* theta_share = m_Param.getWeight();
	// state transition is independent of time t and training set 
	for (size_t z = 0; z < m_topic_size; z++) {
		vector<StateParam>::iterator iter = m_Param.m_StateIndex.begin();
		for (; iter != m_Param.m_StateIndex.end(); ++iter) {
			pair<size_t, size_t> key1 = make_pair(z, iter->y1);
			pair<size_t, size_t> key2 = make_pair(z, iter->y2);
			
			if (m_Mapping.find(key1) == m_Mapping.end() || m_Mapping.find(key2) == m_Mapping.end())
				continue;
			size_t y1 = m_Mapping[key1];
			size_t y2 = m_Mapping[key2];
			m_M[z][ZMAT2(z, y1, y2)] *= exp(theta_share[iter->fid] * iter->fval);	 
		}	
	}
	
}

/**	Calculate the factors.
	References 
		Jeong and Lee, Triangular-chain Conditional Random Fields, IEEE TASLP.
*/
void TriCRF3::calculateFactors(TriStringSequence &triseq) {
	/// Initialization
	m_seq_size = triseq.seq.size() + 1;	///< sequence length
	vector<double*> theta_seq;
	for (size_t z = 0; z < m_topic_size; z++) 
		theta_seq.push_back(m_ParamSeq[z].getWeight());
	double* theta_topic = m_ParamTopic.getWeight();
	double* theta_share = m_Param.getWeight();
	
	m_R.resize(m_topic_size);
	for (size_t z = 0; z < m_topic_size; z++) {
		m_R[z].resize(m_seq_size * m_state_size[z]);
		fill(m_R[z].begin(), m_R[z].end(), 1.0);
	}

	/// Calculation
	for (size_t z = 0; z < m_topic_size; z++) {
		for (size_t i = 0; i < m_seq_size-1; i++) {
			/// Observation factor
			vector<ObsParam> obs_param = m_ParamSeq[z].makeObsIndex(triseq.seq[i].obs);
			vector<ObsParam>::iterator iter = obs_param.begin();
			for(; iter != obs_param.end(); ++iter) {
				m_R[z][ZMAT2(z, i, iter->y)] *= exp(theta_seq[z][iter->fid] * iter->fval);
			}
			

			obs_param = m_Param.makeObsIndex(triseq.seq[i].obs);
			iter = obs_param.begin();
			for(; iter != obs_param.end(); ++iter) {
				pair<size_t, size_t> key = make_pair(z, iter->y);
				if (m_Mapping.find(key) == m_Mapping.end())
					continue;
				size_t y = m_Mapping[key];
				m_R[z][ZMAT2(z, i, y)] *= exp(theta_share[iter->fid] * iter->fval);
			}
			
			

		}	///< for 
	} ///< for each z

	/// Gamma 
	m_Gamma.resize(m_topic_size, 1.0);
	fill(m_Gamma.begin(), m_Gamma.end(), 1.0);
	
	vector<ObsParam> obs_param = m_ParamTopic.makeObsIndex(triseq.topic.obs);
	vector<ObsParam>::iterator iter2 = obs_param.begin();
	for(; iter2 != obs_param.end(); ++iter2) {
		m_Gamma[iter2->y] *= exp(theta_topic[iter2->fid] * iter2->fval);
	}
}

/**	Forward Recursion.
	Computing and storing the alpha value.
*/
void TriCRF3::forward() {
	m_Alpha.resize(m_topic_size);
	for (size_t z = 0; z < m_topic_size; z++) {
		m_Alpha[z].resize(m_seq_size * m_state_size[z]);
		fill(m_Alpha[z].begin(), m_Alpha[z].end(), 0.0);
	}

	for (size_t z = 0; z < m_topic_size; z++) {
		for (size_t j = 0; j < m_state_size[z]; j++) {
				m_Alpha[z][ZMAT2(z, 0, j)] += m_R[z][ZMAT2(z, 0, j)] * m_M[z][ZMAT2(z, m_default_oid, j)]; 
		}
	}

	for (size_t z = 0; z < m_topic_size; z++) {
		for (size_t i = 1; i < m_seq_size; i++) {
			for (size_t j = 0; j < m_state_size[z]; j++) {
				long double prob = m_R[z][ZMAT2(z, i, j)]; 
				
				if (prob > 0) {
					for (size_t k = 0; k < m_state_size[z]; k++) {
							m_Alpha[z][ZMAT2(z, i, j)] += m_Alpha[z][ZMAT2(z, i-1, k)] * m_M[z][ZMAT2(z, k, j)] * prob;
					}
				}
			}
		}
	}
}

/**	Backward Recursion.
	Computing and storing the beta value.
*/
void TriCRF3::backward() {
	m_Beta.resize(m_topic_size);
	for (size_t z = 0; z < m_topic_size; z++) {
		m_Beta[z].resize(m_seq_size * m_state_size[z]);
		fill(m_Beta[z].begin(), m_Beta[z].end(), 0.0);
	}

	/// initializing
	for (size_t z = 0; z < m_topic_size; z++) {
		m_Beta[z][ZMAT2(z, m_seq_size-1, m_default_oid)] = 1.0;
	}

	///for (size_t z = 0; z < m_topic_size; z++) {
	for (size_t prune = 0; prune < m_prune.size(); prune++) {
		size_t z = m_prune[prune].second;

	    for (size_t i = m_seq_size-1; i >= 1; i--) {
		    for (size_t k = 0; k < m_state_size[z]; k++) {
				long double prob = m_R[z][ZMAT2(z, i, k)]; 
				if (prob > 0) {
					for (size_t j = 0; j < m_state_size[z]; j++) {
							m_Beta[z][ZMAT2(z, i-1, j)] += m_Beta[z][ZMAT2(z, i, k)] * m_M[z][ZMAT2(z, j, k)] * prob;
					}
				}
            }
        }
    }
}

/**	Partition function (Z).
	@return normalizing constant 
*/
long double TriCRF3::getPartitionZ() {
	m_prune.clear();
	long double zval = 0.0;

	for (size_t z = 0; z < m_topic_size; z++) {
		long double prob = m_Alpha[z][ZMAT2(z, m_seq_size-1, m_default_oid)] * m_Gamma[z];
		zval += prob;
		m_prune.push_back(make_pair(prob, z));
	}
	/// for pruning
	for (size_t z = 0; z < m_topic_size; z++) {
		m_prune[z].first /= zval;
	}
	sort(m_prune.rbegin(), m_prune.rend());

	return zval;
}

/** Calculate prob. of y* sequence.
	@param seq			given data (y, x)
	@return probability
*/
long double TriCRF3::calculateProb(TriStringSequence& triseq) {
	long double zval = getPartitionZ();

    long double seq_prob = 1.0;
    size_t prev_y = m_default_oid;
    size_t y;
	size_t z = triseq.topic.label;
    for (size_t i=0; i < m_seq_size; i++) {
        if (i < m_seq_size-1) {
            y = triseq.seq[i].label;
        } else {
            y = m_default_oid;
        }
        seq_prob *= m_R[z][ZMAT2(z, i,y)] * m_M[z][ZMAT2(z, prev_y, y)]; 
        prev_y = y;
       
    }
    if (seq_prob == 0.0) {
        cerr << "seq_prob==0 ";
    }

    return seq_prob * m_Gamma[z] / zval;
}

/** Viterbi search to find the best probable output sequence.
  Viterbi algorithm.
 @param prob		dummy probability vector
 @return outcome sequence
*/
vector<size_t> TriCRF3::viterbiSearch(size_t& max_z, long double& prob) {
	/// Initialization
	long double max_prob = -10000.0;
	max_z = m_default_oid;
	vector<size_t> max_y;
	vector<vector<size_t> > psi;
    vector<vector<long double> > delta;

	/// Search
	///for (size_t z = 0; z < m_topic_size; z++) {
	for (size_t prune = 0; prune < m_prune.size(); prune++) {
		size_t z = m_prune[prune].second;

		delta.clear();
		psi.clear();
		for (size_t i=0; i < m_seq_size; i++) {
			vector<size_t> psi_i;
			vector<long double> delta_i;
			for (size_t j=0; j < m_state_size[z]; j++) {
				long double max = -10000.0;
				size_t max_k = 0;
				if (i == 0) {
					max = m_R[z][ZMAT2(z, i, j)] * m_M[z][ZMAT2(z, m_default_oid, j)]; 
					max_k = m_default_oid;
				} else {
					for (size_t k=0; k < m_state_size[z]; k++) {
						double val = delta[i-1][k] * m_R[z][ZMAT2(z, i, j)] * m_M[z][ZMAT2(z, k, j)]; 
						if (val > max) {
							max = val;
							max_k = k;
						}
					}
				}

				delta_i.push_back(max);
				psi_i.push_back(max_k);
			}
			delta.push_back(delta_i);
			psi.push_back(psi_i);

		} ///< for each i

		/// Back-tracking
		vector<size_t> y_seq;
		size_t prev_y = m_default_oid;
		for (size_t i = m_seq_size-1; i >= 1; i--) {
			size_t y = psi[i][prev_y];
			y_seq.push_back(y);
			prev_y = y;
		}
		reverse(y_seq.begin(), y_seq.end());
		double tmp_prob = delta[m_seq_size-1][m_default_oid] * m_Gamma[z];
		
		if (tmp_prob > max_prob) {
			max_prob = tmp_prob;
			max_z = z;
			max_y = y_seq;
		}
	} ///< for each z
	
	prob = max_prob;
	return max_y;

}


/** Training with LBFGS optimizer.
	@param max_iter	maximum number of iteration
	@param sigma	Gaussian prior variance
*/
bool TriCRF3::estimateWithLBFGS(size_t max_iter, double sigma, bool L1, double eta) {
	LBFGS lbfgs;	///< LBFGS optimizer

	/// Parameter weight setting
	size_t n_theta = m_ParamTopic.size();
	double* theta_topic = m_ParamTopic.getWeight();
	vector<double*> theta_seq;
	for (size_t z = 0; z < m_topic_size; z++) {
		theta_seq.push_back(m_ParamSeq[z].getWeight());
		n_theta += m_ParamSeq[z].size();
	}
	double* theta_share = m_Param.getWeight();
	n_theta += m_Param.size();
	
	
	double* theta = new double[n_theta];
	size_t tmp_i = 0;
	for (; tmp_i < m_ParamTopic.size(); ++tmp_i) 
		theta[tmp_i] = theta_topic[tmp_i];
	for (size_t z = 0; z < m_topic_size; z++) {
		for (size_t i = 0; i < m_ParamSeq[z].size(); ++i, ++tmp_i) 
			theta[tmp_i] = theta_seq[z][i];
	}
	for (size_t i = 0; i < m_Param.size(); ++i, ++tmp_i)
		theta[tmp_i] = theta_share[i];
		
	assert(tmp_i == n_theta);

	double* gradient = new double[n_theta]; //m_Param.getGradient();
	double* gradient_topic = m_ParamTopic.getGradient();
	vector<double*> gradient_seq;
	for (size_t z = 0; z < m_topic_size; z++) 
		gradient_seq.push_back(m_ParamSeq[z].getGradient());

	double* gradient_share = m_Param.getGradient();
	
	Evaluator eval1(m_ParamTopic, false);		///< Evaluator (topic)
	Evaluator eval2(m_Param);					///< Evaluator (sequence) 
																///< todo; replace with a general evaluator for seq
	timer t;		///< timer

	/// Reporting
	logger->report("[Parameter estimation]\n");
	logger->report("  Method = \t\tLBFGS\n");
	logger->report("  Regularization = \t%s\n", (sigma ? (L1 ? "L1":"L2") : "none"));
	logger->report("  Penalty value = \t%.2f\n\n", sigma);
	logger->report("  >>Parameters for topic features\n");
	m_ParamTopic.print(logger);
	for (size_t z = 0; z < m_topic_size; z++) {
		logger->report("  >>Parameters for %d plane\n", z);
		m_ParamSeq[z].print(logger);
	}
	logger->report("  >>Parameters for common features\n");
	m_Param.print(logger);		
	logger->report("[Iterations]\n");
	logger->report("%4s %15s %8s %8s %8s %8s\n", "iter", "loglikelihood", "acc", "micro-f1", "macro-f1", "sec");
	
	double old_obj = 1e+37;
	int converge = 0;

	double time_for_factor = 0.0;
	double time_for_forward = 0.0;
	double time_for_backward = 0.0;
	double time_for_evaluation = 0.0;
	double time_for_estimating = 0.0;
	
	/// Training iteration
    for (size_t niter = 0 ;niter < (int)max_iter; ++niter) {

		////////////////////////////////////////////////////////////////////////////
		/// Initializing local variables
		////////////////////////////////////////////////////////////////////////////
        timer t2;	///< elapsed time for one iteration
		m_ParamTopic.initializeGradient();	///< gradient vector initialization
		for (size_t z = 0; z < m_topic_size; z++)
			m_ParamSeq[z].initializeGradient();
		m_Param.initializeGradient();

		eval1.initialize();	///< evaluator intialization
		eval2.initialize(); 
		double time_for_inference = 0.0;

		calculateEdge();

		////////////////////////////////////////////////////////////////////////////
		/// for each training set
		////////////////////////////////////////////////////////////////////////////
        vector<TriStringSequence>::iterator it = m_TrainSet.begin();
		vector<double>::iterator count_it = m_TrainSetCount.begin();
        for (; it != m_TrainSet.end(); ++it, ++count_it) {
			double count = *count_it;
			/// Forward-Backward  
			timer stop_watch;
			calculateFactors(*it);
			time_for_factor += stop_watch.elapsed();
			stop_watch.restart();
  			forward();
			time_for_forward += stop_watch.elapsed();
			long double zval = getPartitionZ();

			////////////////////////////////////////////////////////////////////
			/// pruning
			////////////////////////////////////////////////////////////////////
			long double threshold = m_prune[0].first / m_prune_threshold;
			if (niter > 0) {
				vector<pair<long double, size_t> >::iterator pit = m_prune.begin();
				for (; pit != m_prune.end(); pit++) {
					if (pit->first < threshold) {
						m_prune.erase(pit, m_prune.end());
						break;
					}
				}
			}

			stop_watch.restart();
			backward();
			time_for_backward += stop_watch.elapsed();
			/// Evaluation
			stop_watch.restart();
            long double dummy_prob;
			size_t max_z;
			vector<size_t> y_seq = viterbiSearch(max_z, dummy_prob);
			assert(y_seq.size() == it->seq.size());
			time_for_evaluation += stop_watch.elapsed();

			// calculate Y sequence
			long double y_seq_prob = calculateProb(*it);
            if (!finite((double)y_seq_prob)) {
                cerr << "calculateProb:" << y_seq_prob << endl;
            }

			stop_watch.restart();
			size_t prev_outcome = m_default_oid;
			vector<string> reference, hypothesis;
			double fval = it->topic.fval;
			for (size_t i = 0; i < it->seq.size(); ++i) {	 /// for each node in sequence
				
				size_t outcome = it->seq[i].label;
				string outcome_s = m_ParamSeq[it->topic.label].getState().second[outcome];
				string y_seq_s = m_ParamSeq[max_z].getState().second[y_seq[i]];
				reference.push_back(outcome_s);
				hypothesis.push_back(y_seq_s);

				/// calculate the expectation
				/// E[p] - E[~p]

				/// f(y,x)
				///for (size_t z = 0; z < m_topic_size; z++) {
				for (size_t prune = 0; prune < m_prune.size(); prune++) {
					size_t z = m_prune[prune].second;

					vector<ObsParam> obs_param = m_ParamSeq[z].makeObsIndex(it->seq[i].obs);
					for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
							long double prob = m_Alpha[z][ZMAT2(z, i, iter->y)] * m_Beta[z][ZMAT2(z, i, iter->y)] * m_Gamma[z] / zval;
							gradient_seq[z][iter->fid] += prob * iter->fval * count;
					}

					obs_param = m_Param.makeObsIndex(it->seq[i].obs);
					for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
							pair<size_t, size_t> key = make_pair(z, iter->y);
							if (m_Mapping.find(key) == m_Mapping.end())
								continue;
							size_t y = m_Mapping[key];
							long double prob = m_Alpha[z][ZMAT2(z, i, y)] * m_Beta[z][ZMAT2(z, i, y)] * m_Gamma[z] / zval;
							gradient_share[iter->fid] += prob * iter->fval * count;
					}					
				}

				/// f(y,y)
				if (i > 0) {
					///for (size_t z = 0; z < m_topic_size; z++) {
					for (size_t prune = 0; prune < m_prune.size(); prune++) {
						size_t z = m_prune[prune].second;

						vector<StateParam>::iterator iter = m_ParamSeq[z].m_StateIndex.begin();
						for (; iter != m_ParamSeq[z].m_StateIndex.end(); ++iter) {
							long double a_y;
							long double prob_sum = 0.0;
							if (i == 0) {
								if (iter->y1 == m_default_oid) a_y = 1.0;
								else a_y = 0.0;
							} else {
								a_y = m_Alpha[z][ZMAT2(z, i-1, iter->y1)];
							}
							long double b_y = m_Beta[z][ZMAT2(z, i, iter->y2)];
							long double m_yy = m_R[z][ZMAT2(z, i, iter->y2)] * m_M[z][ZMAT2(z, iter->y1,iter->y2)];
							long double prob = a_y * b_y * m_yy * m_Gamma[z] / zval;
							gradient_seq[z][iter->fid] += prob * iter->fval * count;
						} ///< for each edge
						
						
						iter = m_Param.m_StateIndex.begin();
						for (; iter != m_Param.m_StateIndex.end(); ++iter) {
							pair<size_t, size_t> key1 = make_pair(z, iter->y1);
							pair<size_t, size_t> key2 = make_pair(z, iter->y2);
			
							if (m_Mapping.find(key1) == m_Mapping.end() || m_Mapping.find(key2) == m_Mapping.end())
								continue;
							size_t y1 = m_Mapping[key1];
							size_t y2 = m_Mapping[key2];
							
							long double a_y;
							long double prob_sum = 0.0;
							if (i == 0) {
								if (iter->y1 == m_default_oid) a_y = 1.0;
								else a_y = 0.0;
							} else {
								a_y = m_Alpha[z][ZMAT2(z, i-1, y1)];
							}
							long double b_y = m_Beta[z][ZMAT2(z, i, y2)];
							long double m_yy = m_R[z][ZMAT2(z, i, y2)] * m_M[z][ZMAT2(z, y1, y2)];
							long double prob = a_y * b_y * m_yy * m_Gamma[z] / zval;
							gradient_share[iter->fid] += prob * iter->fval * count;
						} ///< for each edge


					} ///< for z
				}	///< if ( i > 0)
				prev_outcome = outcome;
				
			} ///< for each node in sequence
			
			/// f(z,x)
			vector<ObsParam> obs_param = m_ParamTopic.makeObsIndex(it->topic.obs);
			for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
				long double prob = m_Alpha[iter->y][ZMAT2(iter->y, m_seq_size-1, m_default_oid)] * m_Gamma[iter->y] / zval;
				gradient_topic[iter->fid] += prob * iter->fval * count;
			}
			
			for (size_t c = 0; c < count; c++) {
				eval2.addLikelihood(y_seq_prob);	/// loglikelihood
				eval2.append(m_Param, reference, hypothesis);	/// evaluation (accuracy and f1 score)
				vector<size_t> reference1, hypothesis1;
				reference1.push_back(it->topic.label);
				hypothesis1.push_back(max_z);
				eval1.addLikelihood(y_seq_prob);	/// loglikelihood
				eval1.append(reference1, hypothesis1);
			}
			time_for_estimating += stop_watch.elapsed();
			
		} ///< for m_TrainSet

		////////////////////////////////////////////////////////////////////////////
		/// Parameter Merging
		////////////////////////////////////////////////////////////////////////////
		size_t tmp_i = 0;
		for (; tmp_i < m_ParamTopic.size(); ++tmp_i) {
			gradient[tmp_i] = gradient_topic[tmp_i];
		}
		for (size_t z = 0; z < m_topic_size; z++) {
			for (size_t i = 0; i < m_ParamSeq[z].size(); ++i, ++tmp_i) {
				gradient[tmp_i] = gradient_seq[z][i];
			}
		}
		for (size_t i = 0; i < m_Param.size(); ++i, ++tmp_i) {
			gradient[tmp_i] = gradient_share[i];
		}
		
		assert(n_theta == tmp_i);

		////////////////////////////////////////////////////////////////////////////
		/// Applying regularization
		////////////////////////////////////////////////////////////////////////////
		size_t n_nonzero = 0;
		if (sigma) {
			if (L1) { /// L1 regularization
				for (size_t i = 0; i < n_theta; ++i) {
					eval2.subLoglikelihood(abs(theta[i] / sigma));
					eval1.subLoglikelihood(abs(theta[i] / sigma));
					if (theta[i] != 0.0) 
						n_nonzero++;
				}
			}
			else {	/// L2 regularization
				n_nonzero = n_theta;
				for (size_t i = 0; i < n_theta; ++i) {
					gradient[i] += theta[i] / sigma;
					eval2.subLoglikelihood((theta[i] * theta[i]) / (2 * sigma));
					eval1.subLoglikelihood((theta[i] * theta[i]) / (2 * sigma));
				}
			}
        }
		
		////////////////////////////////////////////////////////////////////////////
		/// Checking the end condition 
		////////////////////////////////////////////////////////////////////////////
		double diff = (niter == 0 ? 1.0 : abs(old_obj - eval2.getObjFunc()) / old_obj);
		if (diff < eta) 
			converge++;
		else
			converge = 0;
		old_obj = eval2.getObjFunc();
		if (converge == 3)
			break;

		////////////////////////////////////////////////////////////////////////////
		/// LBFGS optimizer
		////////////////////////////////////////////////////////////////////////////
		int ret = lbfgs.optimize(n_theta, theta, eval2.getObjFunc(), gradient, L1, sigma);
		if (ret < 0)
			return false;
		else if (ret == 0)
			return true;

		////////////////////////////////////////////////////////////////////////////
		/// Reporting the results
		////////////////////////////////////////////////////////////////////////////
		eval1.calculateF1();
		eval2.calculateF1();
			logger->report("%4d %15E %8.3f %8.3f %8.3f %8.3f\n", niter, eval1.getLoglikelihood(), 
				eval1.getAccuracy(), eval1.getMicroF1()[2], eval1.getMacroF1()[2], t2.elapsed());
			logger->report("%4s %15s %8.3f %8.3f %8.3f %8.3f\n", "", "", 
				eval2.getAccuracy(), eval2.getMicroF1()[2], eval2.getMacroF1()[2], t2.elapsed());

		////////////////////////////////////////////////////////////////////////////
		/// Update the parameter vectors
		////////////////////////////////////////////////////////////////////////////
		m_ParamTopic.setWeight(theta);
		size_t tmp_z = m_ParamTopic.size();
		for (size_t z = 0; z < m_topic_size; z++) {
			m_ParamSeq[z].setWeight(&theta[tmp_z]);
			tmp_z += m_ParamSeq[z].size();
		}
		m_Param.setWeight(&theta[tmp_z]);

	} ///< for iter
	
	delete theta;
	delete gradient;

	return true;
}

/** Training with Psuedo-likelihood.
	@param max_iter	maximum number of iteration
	@param sigma	Gaussian prior variance
*/
bool TriCRF3::estimateWithPL(size_t max_iter, double sigma, bool L1, double eta) {
	LBFGS lbfgs1, lbfgs2;	///< LBFGS optimizer

	/// Parameter weight setting
	size_t n_theta = 0;
	double* theta_topic = m_ParamTopic.getWeight();
	vector<double*> theta_seq;
	for (size_t z = 0; z < m_topic_size; z++) {
		theta_seq.push_back(m_ParamSeq[z].getWeight());
		n_theta += m_ParamSeq[z].size();
	}
	double* theta_share = m_Param.getWeight();
	n_theta += m_Param.size();
	
	double* theta = new double[n_theta];
	size_t tmp_i = 0;
	for (size_t z = 0; z < m_topic_size; z++) {
		for (size_t i = 0; i < m_ParamSeq[z].size(); ++i, ++tmp_i) 
			theta[tmp_i] = theta_seq[z][i];
	}
	for (size_t i = 0; i < m_Param.size(); i++, ++tmp_i)
		theta[tmp_i] = theta_share[i];
		
	assert(tmp_i == n_theta);

	double* gradient = new double[n_theta]; //m_Param.getGradient();
	double* gradient_topic = m_ParamTopic.getGradient();
	vector<double*> gradient_seq;
	for (size_t z = 0; z < m_topic_size; z++) 
		gradient_seq.push_back(m_ParamSeq[z].getGradient());
	double* gradient_share = m_Param.getGradient();
	
	Evaluator eval1(m_ParamTopic, false);		///< Evaluator (topic)
	Evaluator eval2(m_Param);					///< Evaluator (sequence) 
																///< todo; replace with a general evaluator for seq
	timer t;		///< timer

	/// Reporting
	logger->report("[Parameter estimation]\n");
	logger->report("  Method = \t\tPeudolikelihood\n");
	logger->report("  Regularization = \t%s\n", (sigma ? (L1 ? "L1":"L2") : "none"));
	logger->report("  Penalty value = \t%.2f\n\n", sigma);
	logger->report("  >>Parameters for topic features\n");
	m_ParamTopic.print(logger);
	for (size_t z = 0; z < m_topic_size; z++) {
		logger->report("  >>Parameters for %d plane\n", z);
		m_ParamSeq[z].print(logger);
	}
	logger->report("  >>Parameters for common features\n");
	m_Param.print(logger);	
	logger->report("[Iterations]\n");
	logger->report("%4s %15s %8s %8s %8s %8s\n", "iter", "loglikelihood", "acc", "micro-f1", "macro-f1", "sec");
	
	double old_obj = 1e+37, old_obj2 = 1e+37;
	int converge = 0, converge2 = 0;

	double time_for_factor = 0.0;
	double time_for_forward = 0.0;
	double time_for_backward = 0.0;
	double time_for_evaluation = 0.0;
	double time_for_estimating = 0.0;

	/// Training iteration
    for (size_t niter = 0 ;niter < (int)max_iter; ++niter) {

		////////////////////////////////////////////////////////////////////////////
		/// Initializing local variables
		////////////////////////////////////////////////////////////////////////////
        timer t2;	///< elapsed time for one iteration
		m_ParamTopic.initializeGradient();	///< gradient vector initialization
		for (size_t z = 0; z < m_topic_size; z++)
			m_ParamSeq[z].initializeGradient();
		m_Param.initializeGradient();

		eval1.initialize();	///< evaluator intialization
		eval2.initialize(); 
		double time_for_inference = 0.0;

		////////////////////////////////////////////////////////////////////////////
		/// for each training set
		////////////////////////////////////////////////////////////////////////////
        vector<TriStringSequence>::iterator it = m_TrainSet.begin();
		vector<double>::iterator count_it = m_TrainSetCount.begin();
        for (; it != m_TrainSet.end(); ++it, ++count_it) {
			double count = *count_it;

			/////////////////////////////////////////////////////////////////////
			/// PL for topic
			/////////////////////////////////////////////////////////////////////
			vector<size_t> reference1, hypothesis1;
			size_t max_z = 0;
			vector<double> prob_topic(m_topic_size);
			fill(prob_topic.begin(), prob_topic.end(), 0.0);

			/// Inference
			vector<ObsParam> obs_param = m_ParamTopic.makeObsIndex(it->topic.obs);
			for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
				prob_topic[iter->y] += theta_topic[iter->fid] * iter->fval;
			}
		
			/// normalize
			double sum = 0.0;
			double max = 0.0;
			for (size_t j=0; j < m_topic_size; j++) {
				prob_topic[j] = exp(prob_topic[j]); // * y_prob[j];
				sum += prob_topic[j];
				if (prob_topic[j] > max) {
					max = prob_topic[j];
					max_z = j;
				}
			}
			for (size_t j=0; j < m_topic_size; j++) {
				prob_topic[j] /= sum;
			}
			
			/// evaluating
			reference1.push_back(it->topic.label);
			hypothesis1.push_back(max_z);

			/// calculate the expectation
			/// E[p] - E[~p]
			for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
				gradient_topic[iter->fid] += prob_topic[iter->y] * iter->fval * count;
			}

			/////////////////////////////////////////////////////////////////////
			/// PL for sequence
			/////////////////////////////////////////////////////////////////////
			size_t prev_label = m_default_oid;
			size_t next_label = m_default_oid;
			vector<string> reference2, hypothesis2;
			for (size_t i = 0; i < it->seq.size(); ++i) {

				size_t max_y = m_default_oid;
				vector<double> prob_seq(m_state_size[it->topic.label]);
				fill(prob_seq.begin(), prob_seq.end(), 0.0);

				/// w * f (for all classes)
				vector<ObsParam> obs_param = m_ParamSeq[it->topic.label].makeObsIndex(it->seq[i].obs);
				for (vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
					prob_seq[iter->y] += theta_seq[it->topic.label][iter->fid] * iter->fval;
				}
				obs_param = m_Param.makeObsIndex(it->seq[i].obs);
				for (vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
					pair<size_t, size_t> key = make_pair(it->topic.label, iter->y);
					if (m_Mapping.find(key) == m_Mapping.end()) 
						continue;
					prob_seq[m_Mapping[key]] += theta_share[iter->fid] * iter->fval;
				}
				
				for (vector<StateParam>::iterator iter = m_ParamSeq[it->topic.label].m_StateIndex.begin(); iter != m_ParamSeq[it->topic.label].m_StateIndex.end(); ++iter) {
					if (iter->y1 == prev_label)
						prob_seq[iter->y2] += theta_seq[it->topic.label][iter->fid] * iter->fval;
				}
				
				for (vector<StateParam>::iterator iter = m_Param.m_StateIndex.begin(); iter != m_Param.m_StateIndex.end(); ++iter) {
					pair<size_t, size_t> key1 = make_pair(it->topic.label, iter->y1);
					pair<size_t, size_t> key2 = make_pair(it->topic.label, iter->y2);
			
					if (m_Mapping.find(key1) == m_Mapping.end() || m_Mapping.find(key2) == m_Mapping.end())
						continue;
					size_t y1 = m_Mapping[key1];
					size_t y2 = m_Mapping[key2];
				
					if (y1 == prev_label)
						prob_seq[y2] += theta_share[iter->fid] * iter->fval;
				}
			

				/// normalize
				double sum = 0.0;
				double max = 0.0;
				for (size_t j=0; j < m_state_size[it->topic.label]; j++) {
					prob_seq[j] = exp(prob_seq[j]); // * y_prob[j];
					sum += prob_seq[j];
					if (prob_seq[j] > max) {
						max = prob_seq[j];
						max_y = j;
					}
				}
				for (size_t j=0; j < m_state_size[it->topic.label]; j++) {
					prob_seq[j] /= sum;
				}
				
				string outcome_s = m_ParamSeq[it->topic.label].getState().second[it->seq[i].label];
				string y_seq_s = m_ParamSeq[it->topic.label].getState().second[max_y];
				reference2.push_back(outcome_s);
				hypothesis2.push_back(y_seq_s);

				for (vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
					pair<size_t, size_t> key = make_pair(it->topic.label, iter->y);
					if (m_Mapping.find(key) == m_Mapping.end()) 
						continue;
					gradient_share[iter->fid] += prob_seq[m_Mapping[key]] * iter->fval * count;
				}

				obs_param = m_ParamSeq[it->topic.label].makeObsIndex(it->seq[i].obs);
				for (vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
					gradient_seq[it->topic.label][iter->fid] += prob_seq[iter->y] * iter->fval * count;
				}
				
				for (vector<StateParam>::iterator iter = m_ParamSeq[it->topic.label].m_StateIndex.begin(); iter != m_ParamSeq[it->topic.label].m_StateIndex.end(); ++iter) {
					if (iter->y1 == prev_label)
						gradient_seq[it->topic.label][iter->fid] = prob_seq[iter->y2] * iter->fval * count;
				}
				
				for (vector<StateParam>::iterator iter = m_Param.m_StateIndex.begin(); iter != m_Param.m_StateIndex.end(); ++iter) {
					pair<size_t, size_t> key1 = make_pair(it->topic.label, iter->y1);
					pair<size_t, size_t> key2 = make_pair(it->topic.label, iter->y2);
			
					if (m_Mapping.find(key1) == m_Mapping.end() || m_Mapping.find(key2) == m_Mapping.end())
						continue;
					size_t y1 = m_Mapping[key1];
					size_t y2 = m_Mapping[key2];
				
					if (y1 == prev_label)
						gradient_share[iter->fid] = prob_seq[y2] * iter->fval * count;
				}
				

				/// evaluation (accuracy and f1 score)
				for (size_t c = 0; c < count; c++) {
					eval2.addLikelihood(prob_seq[it->seq[i].label]);	
				}

				prev_label = it->seq[i].label;
			}

			/// evaluation
			for (size_t c = 0; c < count; c++) {
				eval1.addLikelihood(prob_topic[it->topic.label]);	
				eval1.append(reference1, hypothesis1);
				eval2.append(m_Param, reference2, hypothesis2);
			}

		} ///< for m_TrainSet


		////////////////////////////////////////////////////////////////////////////
		/// Parameter Merging
		////////////////////////////////////////////////////////////////////////////
		size_t tmp_i = 0;
		for (size_t z = 0; z < m_topic_size; z++) {
			for (size_t i = 0; i < m_ParamSeq[z].size(); ++i, ++tmp_i) {
				gradient[tmp_i] = gradient_seq[z][i];
			}
		}
		for (size_t i = 0; i < m_Param.size(); i++, ++tmp_i)
			gradient[tmp_i] = gradient_share[i];
		
		assert(n_theta == tmp_i);

		////////////////////////////////////////////////////////////////////////////
		/// Applying regularization
		////////////////////////////////////////////////////////////////////////////
		size_t n_nonzero = 0;
		if (sigma) {
			if (L1) { /// L1 regularization
				for (size_t i = 0; i < m_ParamTopic.size(); ++i) {
					eval1.subLoglikelihood(abs(theta_topic[i] / sigma));
					if (theta_topic[i] != 0.0) 
						n_nonzero++;
				}
				for (size_t i = 0; i < n_theta; ++i) {
					eval2.subLoglikelihood(abs(theta[i] / sigma));
					if (theta[i] != 0.0) 
						n_nonzero++;
				}
			}
			else {	/// L2 regularization
				n_nonzero = n_theta + m_ParamTopic.size();
				for (size_t i = 0; i < m_ParamTopic.size(); ++i) {
					gradient_topic[i] += theta_topic[i] / sigma;
					eval1.subLoglikelihood((theta_topic[i] * theta_topic[i]) / (2 * sigma));
				}
				for (size_t i = 0; i < n_theta; ++i) {
					gradient[i] += theta[i] / sigma;
					eval2.subLoglikelihood((theta[i] * theta[i]) / (2 * sigma));
				}
			}
        }
		
		////////////////////////////////////////////////////////////////////////////
		/// Checking the end condition 
		////////////////////////////////////////////////////////////////////////////
		double diff = (niter == 0 ? 1.0 : abs(old_obj - eval1.getObjFunc()) / old_obj);
		if (diff < eta) 
			converge++;
		else
			converge = 0;
		old_obj = eval1.getObjFunc();
		if (converge == 3)
			break;

		double diff2 = (niter == 0 ? 1.0 : abs(old_obj2 - eval2.getObjFunc()) / old_obj2);
		if (diff2 < eta) 
			converge2++;
		else
			converge2 = 0;
		old_obj2 = eval2.getObjFunc();
		if (converge2 == 3)
			break;

		////////////////////////////////////////////////////////////////////////////
		/// LBFGS optimizer
		////////////////////////////////////////////////////////////////////////////
		//int ret = lbfgs.optimize(n_theta, theta, eval2.getObjFunc(), gradient, L1, sigma);
		int ret = lbfgs1.optimize(m_ParamTopic.size(), theta_topic, eval1.getObjFunc(), gradient_topic, L1, sigma);
		if (ret < 0)
			return false;
		else if (ret == 0)
			return true;
		ret = lbfgs2.optimize(n_theta, theta, eval2.getObjFunc(), gradient, L1, sigma);
		if (ret < 0)
			return false;
		else if (ret == 0)
			return true;

		////////////////////////////////////////////////////////////////////////////
		/// Reporting the results
		////////////////////////////////////////////////////////////////////////////
		eval1.calculateF1();
		eval2.calculateF1();
			logger->report("%4d %15E %8.3f %8.3f %8.3f %8.3f\n", niter, eval1.getLoglikelihood(), 
				eval1.getAccuracy(), eval1.getMicroF1()[2], eval1.getMacroF1()[2], t2.elapsed());
			logger->report("%4s %15E %8.3f %8.3f %8.3f %8.3s\n", "", eval2.getLoglikelihood(), 
				eval2.getAccuracy(), eval2.getMicroF1()[2], eval2.getMacroF1()[2], "");

		////////////////////////////////////////////////////////////////////////////
		/// Update the parameter vectors
		////////////////////////////////////////////////////////////////////////////
		//m_ParamTopic.setWeight(theta);
		size_t tmp_z = 0; //m_ParamTopic.size();
		for (size_t z = 0; z < m_topic_size; z++) {
			m_ParamSeq[z].setWeight(&theta[tmp_z]);
			tmp_z += m_ParamSeq[z].size();
		}
		m_Param.setWeight(&theta[tmp_z]);

	} ///< for iter
	
	delete theta;
	delete gradient;

	return true;
}

bool TriCRF3::pretrain(size_t max_iter, double sigma, bool L1) { 
	return estimateWithPL(max_iter, sigma, L1);
}

bool TriCRF3::train(size_t max_iter, double sigma, bool L1) { 
	return estimateWithLBFGS(max_iter, sigma, L1); 
}

bool TriCRF3::test(const std::string& filename, const std::string& outputfile, bool confidence) {
	/// File stream
	string line;
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");

	/// output
	ofstream out;
	//vector<string> state_vec;
	if (outputfile != "") {
		out.open(outputfile.c_str());
		out.precision(20);
		//state_vec = m_ParamTopic.getState().second;
	}
	/*	
	ofstream out, outs[m_ParamTopic.sizeStateVec()];
	if (outputfile != "") {
		string name = outputfile + ".TOPIC";
		out.open(name.c_str());
		out.precision(20);
		for (size_t i = 0; i < m_ParamTopic.sizeStateVec(); i++) {		
			string name = outputfile + "." + m_ParamTopic.getState().second[i];
			outs[i].open(name.c_str());
			outs[i].precision(20);
		}			
	}		
	*/	

	/// initializing
	size_t count = 0;
	TriStringSequence triseq;
	logger->report("[Testing begins ...]\n");
	timer stop_watch;
	Evaluator test_eval1(m_ParamTopic, false);		///< Evaluator (topic)
	Evaluator test_eval2(m_Param);		///< Evaluator (sequence)
	test_eval1.initialize();	///< evaluator intialization
	test_eval2.initialize(); 
	
	/////// for MULTI-DOMAIN SLU evaluation 2008. 4. 30
	Evaluator evals[m_ParamTopic.sizeStateVec()];
	for (size_t i = 0; i < m_ParamTopic.sizeStateVec(); i++) {
		evals[i].encode(m_ParamSeq[i]);
		evals[i].initialize();
	}
	
	size_t seq_count = 0;
	
	calculateEdge();
	
	/// reading the text
	while (getline(f,line)) {
		vector<string> tokens = tokenize(line, " \t");
		if (line.empty()) {
			/// test
			calculateFactors(triseq);
  			forward();
			long double zval = getPartitionZ();
            long double dummy_prob;

			////////////////////////////////////////////////////////////////////
			/// pruning
			////////////////////////////////////////////////////////////////////
			long double threshold = m_prune[0].first / m_prune_threshold;
			vector<pair<long double, size_t> >::iterator pit = m_prune.begin();
			for (; pit != m_prune.end(); pit++) {
				if (pit->first < threshold) {
					m_prune.erase(pit, m_prune.end());
					break;
				}
			}

			size_t max_z;
			vector<size_t> y_seq = viterbiSearch(max_z, dummy_prob);
			assert(y_seq.size() == triseq.seq.size());

			vector<size_t> reference1, hypothesis1;
			reference1.push_back(triseq.topic.label);
			hypothesis1.push_back(max_z);
			test_eval1.append(reference1, hypothesis1);
			if (outputfile != "") {
				string outcome_s = m_ParamTopic.getState().second[max_z];			
				out << outcome_s;
				/*
				if (confidence) {
					double prob = m_Alpha[max_z][ZMAT2(max_z, m_seq_size-1, m_default_oid)] * m_Gamma[max_z] / zval;
					out << " " << prob;
				}
				*/
				out << endl;
			}

			size_t prev_y = m_default_oid;
			vector<string> reference, hypothesis;
			StringSequence::iterator it = triseq.seq.begin();
			for (size_t i = 0; it != triseq.seq.end(); ++i, ++it) {	 /// for each node in sequence
				size_t outcome = triseq.seq[i].label;
				string outcome_s;
				/// If there are non-attested labels in dev, test sets, then ...
				if (m_ParamTopic.sizeStateVec() <= triseq.topic.label || m_ParamSeq[triseq.topic.label].sizeStateVec() <= outcome) 
					outcome_s = m_Param.getState().second[m_default_oid];
				else
					outcome_s = m_ParamSeq[triseq.topic.label].getState().second[outcome];
				string y_seq_s = m_ParamSeq[max_z].getState().second[y_seq[i]];

				reference.push_back(outcome_s);
				hypothesis.push_back(y_seq_s);

				if (outputfile != "") {
					//outs[triseq.topic.label] << y_seq_s << endl;
					out << y_seq_s << endl;
					/*
					if (confidence) {
						double norm = 0.0;
						for (size_t j = 0; j < m_state_size[max_z]; j++)
							norm += m_R[max_z][ZMAT2(max_z, i, j)] * m_M[max_z][ZMAT2(max_z, prev_y,j)]; 
						double prob = m_R[max_z][ZMAT2(max_z, i, y_seq[i])] * m_M[max_z][ZMAT2(max_z, prev_y,y_seq[i])] / norm;
						out << " " << prob;
						prev_y = y_seq[i];
					}
					*/
					//outs[triseq.topic.label] << endl; 
				}
			}
			if (outputfile != "")
				out << endl; 
				//outs[triseq.topic.label] << endl; 

			test_eval2.append(m_Param, reference, hypothesis);
			evals[triseq.topic.label].append(m_ParamSeq[triseq.topic.label], reference, hypothesis);		

			triseq.seq.clear();
			seq_count = 0;
			++count;
		} else {
			++seq_count;
			if (seq_count == 1) { ///< this is a topic 
				triseq.topic = packEvent(tokens, &m_ParamTopic, true);	///< wanrning: There are no common element in topic classes and sequence classes.
			} else {
				size_t z = (triseq.topic.label < m_ParamTopic.sizeStateVec() ? triseq.topic.label : m_default_oid);
				StringEvent ev = packStringEvent(tokens,  &m_ParamSeq[z], true);	///< observation features
				triseq.seq.push_back(ev);	///< append
			}

		}	///< else
	}	///< while

	test_eval1.calculateF1();
	test_eval2.calculateF1();
	for (size_t i = 0; i < m_ParamTopic.sizeStateVec(); i++) 
		evals[i].calculateF1();	
	
	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  testing time = \t%.3f\n\n", stop_watch.elapsed());
	logger->report("[Topic Classification]\n");
	logger->report("  Acc = \t\t%8.3f\n", test_eval1.getAccuracy());
	logger->report("  MicroF1 = \t\t%8.3f\n", test_eval1.getMicroF1()[2]);
	logger->report("  MacroF1 = \t\t%8.3f\n", test_eval1.getMacroF1()[2]);
	test_eval1.Print(logger);

	logger->report("[Sequential Labeling]\n");
	logger->report("  Acc = \t\t%8.3f\n", test_eval2.getAccuracy());
	logger->report("  MicroF1 = \t\t%8.3f\n", test_eval2.getMicroF1()[2]);
	logger->report("  MacroF1 = \t\t%8.3f\n", test_eval2.getMacroF1()[2]);
	test_eval2.Print(logger);
	logger->report("\n-------------PER TOPIC CLASS-------------------------------------------\n");
	for (size_t i = 0; i < m_ParamTopic.sizeStateVec(); i++) {
		logger->report("%s MicroF1 = \t\t%8.3f\n", m_ParamTopic.getState().second[i].c_str(), evals[i].getMicroF1()[2]);	
		logger->report("- Domain = %s ----------------------------------------------------\n", m_ParamTopic.getState().second[i].c_str());
		evals[i].Print(logger);
	}
	
}

}	///< namespace tricrf









